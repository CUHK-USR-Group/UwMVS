
---
permalink: /demo
layout: default
title: "Demo for the paper"
---

Content related to demos will be placed here.


---

## Introduction
\IEEEPARstart{M}{ulti-View} stereo (MVS) aims to recover the dense 3D representation of the scene leveraging stereo correspondences as the main cue given calibrated 2D images from multiple views (more than two views), essentially equivalent to solving the pixel correspondences across multi-view images. Recently, learning-based MVS approaches~\cite{MVSNet, R-MVSNet, CasMVSNet, Vis-MVSNet, AA-RMVSNet, EPP-MVSNet, CDS-MVSNet, Uni-MVSNet, TransMVSNet} have significantly outperformed the traditional counterparts in MVS benchmarks~\cite{DTU_dataset, Tanks_dataset, ETH3D, BlendedMVS}. Deep MVS approaches decouple the MVS into a two-stage process: \textit{learning-based depth map estimation} and \textit{depth map filtering \& fusion}. Compared to the hand-crafted photometric measures in traditional approaches, deep MVS approaches encode scene cues such as reflective priors and illumination changes into the network by adopting powerful feature extraction and cost volume representation to achieve superior reconstruction accuracy and completeness. Despite the superiority of the learning-based MVS approaches, the following improvements can be made to further boost the overall reconstruction quality: Most recent learning-based methods~\cite{CasMVSNet, UCS-Net, Uni-MVSNet} use feature pyramid network (FPN) to extract multi-scale features for constructing cost volume pyramid. The depth estimation and subsequent reconstruction may suffer from over-smoothing around the object boundaries due to the lack of shallow feature information containing low-level features such as local textures and edges. 
<img src="/images/image/Framework.pdf"/>

---

## Feature Pyramid Extraction
\label{subsec: deep_feature_extraction} Most recent learning-based methods~\cite{CasMVSNet, UCS-Net, Uni-MVSNet, TransMVSNet} adopts the coarse-to-fine depth estimation strategy and utilizes FPN to extract multi-scale image features for constructing cost volumes at different resolutions. As the depth estimation and subsequent reconstruction may suffer from over-smoothing around the object boundaries due to the lack of shallow feature information containing low-level features such as local textures and edges~\cite{MVSNet}, we enhance the shallow feature information flow by introducing a bottom-up pathway to augment the propagation of low-level features and enlarge the receptive field to incorporate global context information for more accurate and robust feature matching under low-textured regions~\cite{PANet}. We integrate in-place activated batch normalization (ABN) to reduce memory footprint in a computationally efficient way~\cite{InPlace-ABN}. Our feature pyramid extraction network takes as input multi-view images $\{\textbf{I}_{i} \}_{i=0}^{N}$ and output ($L+1$)-level feature pyramids $\{\textbf{f}_{l,i} \in \mathbb{R}^{F_l \times H/2^{l} \times W/2^{l}} \}_{l=0}^{L}$ for each image $\textbf{I}_{i}$, where $l$ represents the level ordinal, $l=L$ is the coarsest level, $l=0$ is the finest level, $F_l$ is the channel number of the feature map at level $l$, $H/{2^l}$ and $W/{2^l}$ is the height and width of the $l$-th level feature map downsampled to $1/2^l$ of the original input image resolution, respectively. Specifically, the...


---

## Benchmark Performance
\label{subsec: benchmark} \textbf{Benchmark on DTU Dataset} We benchmark our method on the \textit{DTU evaluation set} and conduct a comprehensive comparison with traditional (geometric) and state-of-the-art learning-based MVS approaches. We follow the standard evaluation procedure~\cite{DTU_dataset} for quantitative benchmark and summarize the \textit{mean error distance} metrics (in $mm$, lower the better) including reconstruction \textit{accuracy}, \textit{completeness}, and \textit{overall score} as shown in Table~\ref{dtu_benchmark}. With different settings, including the changes of $N$ and $N_c$ (detailed in the Appendix), our method performs an excellent trade-off between the reconstruction \textit{accuracy} and \textit{completeness}. It achieves the best performance in terms of the \textit{accuracy}, \textit{completeness} and \textit{overall score} compared with the existing traditional and learning-based methods, indicating the state-of-the-art performance of our method. We qualitatively compare the depth estimation and reconstruction results of several reflective and low-textured scenes with illumination changes on \textit{DTU evaluation set} in Fig.~\ref{figure: dtu_depth_compare} and Fig.~\ref{figure: dtu_compare} respectively, where our method achieves more complete depth estimation and dense point cloud reconstruction with fine-grained details preserved benefiting from the proposed LCM scheme, qualitatively verifying the quantitative comparison results. \vspace{-1mm} \begin{table}[htbp!] \renewcommand{\arraystretch}{1.3} \caption{Quantitative Benchmarking Results on \textit{DTU Evaluation Set}} \label{dtu_benchmark} \begin{center} \resizebox{0.8\columnwidth}{!}{ \begin{threeparttable} \begin{tabular}{l l c c c} \hline\hline \\[-3mm] \multirow{2}{*}{Type} & \multirow{2}{*}{Methods} & \multicolumn{3}{c}{Mean Error...
<img src="/images/image/TNT-Compare-2.pdf"/>

---

## Real-World Application for UAV-Based Infrastructure Defect Inspection and Localization
\label{sec: uav} As demonstrated in Fig.~\ref{figure: Framework_Overview}, we deploy our MVS method into our UAV-based infrastructure defect inspection system for reconstruction-based defect localization, with crack as our target defect. \textbf{Unmanned Aerial System} As shown in Fig.~\ref{figure: depth_curve} (a), three UAVs are used to speed up the image collection for defect detection and reconstruction of the target warehouse. Each UAV will reach the best region according to the task assignment method. After reaching the best region, viewpoints can be generated based on the building morphology. Furthermore, these viewpoints are regarded as the nodes of a traveling salesman problem to determine the shortest path that travels through all these viewpoints. Finally, the generated path will be executed by each UAV to collect the images for inspection and reconstruction. The multi-UAV-based data collection can speed up the overall image collection process by more than 3 times. \begin{figure}[htbp!]% \centering \includegraphics[width=0.85\columnwidth]{image/defects.pdf} \caption{Experiments for (a) multi-UAV-based data collection; (b) defect detection results.} \label{figure: depth_curve} \vspace{-4mm} \end{figure} \textbf{Defect Detection} Based on our \textit{UAS} system, we collect $923$ images with crack defect from different directions of the target warehouse for defect detection. We train and compare the performance of the state-of-the-art object detection methods on our established defect...
<img src="/images/image/defects.pdf"/>

